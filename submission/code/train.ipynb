{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import default scientific libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas for easy data management, keras for deep learning and scikit for feature generation\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "import keras.optimizers as ko\n",
    "import mca\n",
    "from sklearn import metrics, model_selection, manifold, decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# item count\n",
    "n_i = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# user count\n",
    "n_u = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('train_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = pd.read_csv('test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build categorical label for each rating (train set), e.g. rating of 2 gives [0, 1, 0, 0, 0]\n",
    "train_y = np.zeros([train_x.shape[0], 5])\n",
    "train_y[np.arange(train_x.shape[0]), train_x.Prediction - 1] = 1\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (same for test set)\n",
    "test_y = np.zeros([test_x.shape[0], 5])\n",
    "test_y[np.arange(test_x.shape[0]), test_x.Prediction - 1] = 1\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load needed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = np.load('items.npy')\n",
    "items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items2 = np.load('items2.npy')\n",
    "items2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items3 = np.load('items3.npy')\n",
    "items3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = np.load('users.npy')\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users2 = np.load('users2.npy')\n",
    "users2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users3 = np.load('users3.npy')\n",
    "users3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure tensorflow does not leak\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build net 1\n",
    "def net1():\n",
    "    # normalization is used to increase stability among the net\n",
    "    # dropout avoids overfitting by cutting some connections between the batches\n",
    "    # relu was the best activation function found\n",
    "    # the size of each layer is reducing to converge to the last one\n",
    "    \n",
    "    features = 48\n",
    "    \n",
    "    # input layer for items, embedding two dimensional (features x 10) so it needs to be flatten\n",
    "    input_i = kl.Input(shape=[1])\n",
    "    i = kl.Embedding(n_i + 1, features)(input_i)\n",
    "    i = kl.Flatten()(i)\n",
    "    i = kl.normalization.BatchNormalization()(i)\n",
    "\n",
    "    # input layer for items, embedding two dimensional (features x 10) so it needs to be flatten\n",
    "    input_u = kl.Input(shape=[1])\n",
    "    u = kl.Embedding(n_u + 1, features)(input_u)\n",
    "    u = kl.Flatten()(u)\n",
    "    u = kl.normalization.BatchNormalization()(u)\n",
    "\n",
    "    # input layer for item contexts\n",
    "    input_im = kl.Input(shape=[items.shape[1]])\n",
    "    im = kl.normalization.BatchNormalization()(input_im)\n",
    "    input_im2 = kl.Input(shape=[items2.shape[1]])\n",
    "    im2 = kl.normalization.BatchNormalization()(input_im2)\n",
    "    input_im3 = kl.Input(shape=[users3.shape[1]])\n",
    "    im3 = kl.normalization.BatchNormalization()(input_im3)\n",
    "\n",
    "    # input layer for user contexts\n",
    "    input_um = kl.Input(shape=[users.shape[1]])\n",
    "    um = kl.normalization.BatchNormalization()(input_um)\n",
    "    input_um2 = kl.Input(shape=[users2.shape[1]])\n",
    "    um2 = kl.normalization.BatchNormalization()(input_um2)\n",
    "    input_um3 = kl.Input(shape=[items3.shape[1]])\n",
    "    um3 = kl.normalization.BatchNormalization()(input_um3)\n",
    "\n",
    "    # merge everything together\n",
    "    nn = kl.merge([i, u, im, um, im2, um2, im3, um3], mode='concat')\n",
    "    \n",
    "    # densely connectect layers\n",
    "    nn = kl.Dense(1024, activation='relu')(nn)\n",
    "    nn = kl.Dropout(0.5)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(512, activation='relu')(nn)\n",
    "    nn = kl.Dropout(0.5)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(256, activation='relu')(nn)\n",
    "    nn = kl.Dropout(0.5)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(128, activation='relu')(nn)\n",
    "\n",
    "    # last layer is using softmax to obtain the confidence\n",
    "    output = kl.Dense(5, activation='softmax')(nn)\n",
    "\n",
    "    # optimize using adam and cross entropy among different binary labels\n",
    "    model = km.Model([input_i, input_u, input_im, input_um, input_im2, input_um2, input_im3, input_um3], output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = net1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = model.fit(\n",
    "    [train_x.Item, train_x.User, users[train_x.Item - 1], items[train_x.User - 1], users2[train_x.Item - 1], items2[train_x.User - 1], users3[train_x.Item - 1], items3[train_x.User - 1]], train_y,\n",
    "    validation_data=([test_x.Item, test_x.User, users[test_x.Item - 1], items[test_x.User - 1], users2[test_x.Item - 1], items2[test_x.User - 1], users3[test_x.Item - 1], items3[test_x.User - 1]], test_y),\n",
    "    batch_size=4096,\n",
    "    nb_epoch=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot training loss vs validation loss\n",
    "plt.plot(epochs.history['loss'], label='loss')\n",
    "plt.plot(epochs.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for train set\n",
    "trained = model.predict(\n",
    "    [train_x.Item, train_x.User, users[train_x.Item - 1], items[train_x.User - 1], users2[train_x.Item - 1], items2[train_x.User - 1], users3[train_x.Item - 1], items3[train_x.User - 1]], \n",
    "    batch_size=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for validation set\n",
    "validated = model.predict(\n",
    "    [test_x.Item, test_x.User, users[test_x.Item - 1], items[test_x.User - 1], users2[test_x.Item - 1], items2[test_x.User - 1], users3[test_x.Item - 1], items3[test_x.User - 1]], \n",
    "    batch_size=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rmse choosing highest confidence for each rating\n",
    "np.sqrt(metrics.mean_squared_error(np.argmax(validated, 1) + 1, test_x.Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute weighted mean between with rating confidence and rating\n",
    "def weighted_mean(preds):\n",
    "    ret = []\n",
    "    for e, s in zip(preds, np.argsort(preds, axis=1)):\n",
    "        # highest confidence rating (index of the sorted array)\n",
    "        #               |\n",
    "        #               v\n",
    "        n1, n2, n3, n4, n5 = s\n",
    "        val = (n5 * e[n5] + n4 * e[n4] + n3 * e[n3] + n2 * e[n2] + n1 * e[n1]) / (e[n1] + e[n2] + e[n3] + e[n4] + e[n5])\n",
    "        ret.append(val + 1)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abandonnated net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build net 2\n",
    "def net2():\n",
    "    # normalization is used to increase stability among the net\n",
    "    # dropout avoids overfitting by cutting some connections between the batches\n",
    "    # relu was the best activation function found\n",
    "\n",
    "    # input layer from the previous net\n",
    "    input2 = kl.Input(shape=[5])\n",
    "\n",
    "    # densely connectect layers\n",
    "    nn = kl.Dense(128, activation='relu')(input2)\n",
    "    nn = kl.Dropout(0.2)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(128, activation='relu')(nn)\n",
    "    nn = kl.Dropout(0.2)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(128, activation='relu')(nn)\n",
    "    nn = kl.Dropout(0.2)(nn)\n",
    "    nn = kl.normalization.BatchNormalization()(nn)\n",
    "    nn = kl.Dense(128, activation='relu')(nn)\n",
    "\n",
    "    # output layer is the rating\n",
    "    output = kl.Dense(1, activation='relu')(nn)\n",
    "\n",
    "    # optimize using adam and mse among different ratings\n",
    "    # decaying the optimizer is needed to have a convergence\n",
    "    model2 = km.Model([input2], output)\n",
    "    model2.compile(optimizer=ko.Adam(decay=0.0025), loss='mean_squared_error')\n",
    "    return model2\n",
    "    \n",
    "model2 = net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "epochs2 = model2.fit(\n",
    "    [trained], train_x.Prediction,\n",
    "    validation_data=([validated], test_x.Prediction),\n",
    "    batch_size=4096,\n",
    "    nb_epoch=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for train set\n",
    "trained2 = model2.predict([trained], batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute prediction for validation set\n",
    "validated2 = model2.predict([validated], batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rmse using second net\n",
    "np.sqrt(metrics.mean_squared_error(validated2, test_x.Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
